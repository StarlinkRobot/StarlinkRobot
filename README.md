# ğŸ¤– StarlinkRobot/StarlinkRobot

## ğŸ¤– Starlink Robot

Thank you for your interest in Starlink Robot! ğŸŒŸ

We're excited to share our progress with you.

---

### ğŸ“ Project Homepage

The project homepage is in:

**[ğŸŒ https://starlinkrobot.github.io/](https://starlinkrobot.github.io/)**

Includes the video, datasets, and paper.

---

## ğŸ“– About

**The Starlink Robot Dataset**

### ğŸ” Overview

The Starlink Robot is a mobile platform based on the Unitree Go2 wheeled version, integrating Starlink Mini for satellite connectivity, Livox Mid-360 LiDAR for 3D mapping, an upward-facing fisheye camera for sky visibility, and motion sensors via the robot's internal SLAM. It captures synchronized multi-modal data to study satellite communication under motion and occlusion, as detailed in the SenSys 2026 paper. The dataset includes over 7 hours of data from London urban environments, with 25K+ RTT measurements, 630K LiDAR frames, and 378K fisheye images, expanded with high-speed and late-afternoon sessions for diversity.

- **ğŸ¤ Collaborators**: Ongoing with Norwegian University of Science and Technology (NTNU) and University of Virginia for geographical diversity.
- **ğŸ’¾ Dataset Access**: Hosted on the companion site (add your link here).
- **ğŸ¥ Video Demonstration**: Available on the companion site (add your link here).

---

## âš™ï¸ Installation

1. Clone the repo:
   ```bash
   git clone https://github.com/StarlinkRobot/StarlinkRobot.git
   ```

2. Install Python dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Install ROS Noetic on Ubuntu 18.04 for bag parsing (as per paper Section 3.2).

4. For LEOViz: Clone from https://github.com/clarkzjw/LEOViz and install its dependencies (Python, gRPC).

5. **ğŸ”§ Hardware requirements**: Unitree Go2-W (dimensions 70x43x50cm, payload 8-12kg, weight 18kg), Starlink Mini (weight 1.10kg, IP67, -30Â°C to 50Â°C operating temp), Livox Mid-360 (65x65x60mm, 6.5W, 905nm laser).

---

## ğŸ“ Usage Instructions

Detailed steps for hardware setup, data collection, and analysis, based on paper Section 3 (System Design).

### ğŸ”© Hardware Setup

- Assemble the platform on Unitree Go2 wheeled base (differential drive, velocity 0.1-2.0 m/s, payload â‰ˆ8kg).
- Mount Starlink Mini (weight 1.10kg) on 3D-printed support to maintain upward orientation and reduce vibration.
- Connect Starlink Mini to Intel NUC via Ethernet; use DC-DC converter for power (robot 24V to Starlink 12V).
- Install Livox Mid-360 LiDAR (360Â° x 59Â° FOV, 200,000 points/s, 25 Hz custom rate per paper, 0.05m accuracy) and fisheye camera (185Â° FOV, 15 Hz, 1920x1080 resolution) upward-facing.
- No separate IMU/GPS: Rely on robot's internal SLAM estimator and Starlink's built-in localization for motion data.
- **âš¡ Power**: Platform draws 60-110W (base) + 20-40W (Starlink Mini, 15W idle), supporting ~2 hours operation.

### ğŸ“¡ Data Collection Steps

1. **ğŸ”Œ Power On**: Activate Unitree Go2 and Intel NUC (Ubuntu 18.04). Ensure battery is charged for 2-hour endurance.

2. **ğŸš€ Start ROS**: Run `roscore` and source workspace. Launch sensor drivers:
   ```bash
   roslaunch livox_ros_driver livox_lidar.launch
   ```
   for LiDAR (25 Hz), and camera node for fisheye (15 Hz).

3. **ğŸ—ºï¸ Start SLAM**: Initialize Unitree internal SLAM via SDK for odometry, velocity, and position tracking (sub-ms alignment via NUC clock).

4. **ğŸ›°ï¸ Start LEOViz**: Execute:
   ```bash
   ./scripts/run_leoviz.sh
   ```
   to query Starlink gRPC at 1Hz for satellite tracking (positions, elevation, SNR). Enable JSON logging.

5. **ğŸŒ Network Probing**: Run tools like ping for RTT (20-400ms range), iperf for throughput, logging to CSV with timestamps.

6. **ğŸ’¿ Data Logging**: Use:
   ```bash
   rosbag record -a
   ```
   for raw sensors (LiDAR point clouds, camera images, SLAM odom). Log Starlink/LEOViz to separate CSVs.

7. **ğŸ® Mobility Execution**: Control robot via joystick or SDK for scenarios (steady 0.1-2.0 m/s, variable speeds, open/tree-urban paths). Collect under diverse conditions like high-speed (up to 2 m/s) and occlusions.

8. **ğŸ”„ Post-Processing**: Parse bags with:
   ```bash
   ./scripts/parse_rosbag.py
   ```
   sync via:
   ```bash
   ./scripts/sync_datasets.py
   ```
   (GPS-based alignment), visualize with:
   ```bash
   ./scripts/visualize_metrics.py
   ```
   to reproduce paper figures (e.g., RTT vs. velocity).

9. **ğŸ“Š LEOViz Parsing**: Use:
   ```bash
   ./examples/leoviz_output_parser.py
   ```
   to align JSON logs with metrics.

For dataset details, see [docs/data_structure.md](docs/data_structure.md) and [docs/file_formats.md](docs/file_formats.md). For LEOViz, see [docs/leoviz_integration.md](docs/leoviz_integration.md).

---

## ğŸ“Š Visualization Tool: LEOViz

This work uses the visualization tool [LEOViz](https://github.com/clarkzjw/LEOViz), developed by Jinwei Zhang from the Pan Lab, led by Prof. Jianping Pan at the University of Victoria.

ğŸ™ We sincerely acknowledge their contribution.

ğŸ§‘â€ğŸ¤â€ğŸ§‘ We are planning to work together in future collaborations to further enhance the Starlink Robot project.

---

## ğŸš§ Project Status

Due to the large volume of data, we are continuously improving the system and dataset.

---

## ğŸ“„ License

MIT License. Acknowledgments: LEOViz developers; Unitree, Livox, Starlink vendors.